{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# No free lunch theorems for supervised learning\n",
    "   **Elvis Sikora** \n",
    "   \n",
    "   elvis.sikora@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Contents:**\n",
    "1. Informal discussion about theory and induction\n",
    "2. Shwartz & David's NFLT + rudiments of PAC-Learning\n",
    "3. Wolpert's NFLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Why should we even care? What is this all about?\n",
    "**tl;dr:** _there's no universally superior learner, but this might not be super relevant in practice_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Statistical Learning Theory (SLT)\n",
    "SLT is about \n",
    "_fundamental_ rather than _practical_ questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The job of SLT is not to:\n",
    "* provide tricks to accelerate training a neural net\n",
    "* help us achieve SotA in any specific dataset or learning task\n",
    "* make the results of learning algorithms more interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The kind of questions SLT helps us address\n",
    "* what is learning?\n",
    "* is it even possible to learn from finite datasets?\n",
    "* how (computationally) hard is learning?\n",
    "* what fundamental tradeoffs are involved in learning?\n",
    "* is there a universally (across all conceivable learning tasks) superior learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These questions are philosophical, there might be no _one correct answer_ to them. But SLT provides us with useful ways to reason about them.\n",
    "\n",
    "We'll discuss two theorems that allow us to give a negative answer to the last question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem of induction\n",
    "\n",
    "In machine learning terms: can we learn from finite data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A thought experiment\n",
    "\n",
    "We are tasked with training an ML model to answer the question: \n",
    "_will the sun rise 1/jan next year?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each night, we get out of bed and go take notes on our window:\n",
    "1. on the 1st day, _the sun did rise_\n",
    "2. on the 2nd day, _the sun did rise_\n",
    "3. on the 3rd day, _the sun did rise_\n",
    "4. on the 4th day, _the sun did rise_\n",
    "\n",
    "... and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We collect data for m = 365 days from 1/jan to 31/dec this year.\n",
    "For all 365 examples in our *training set*, the sun did rise.\n",
    "\n",
    "Can we be **sure** about what will happen on 1/jan next year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The depressing answer: no\n",
    "\n",
    "There is no _logic_ reason to say the sun will rise the next day.\n",
    "\n",
    "Anything could happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### The way out\n",
    "\n",
    "We can fix this issue by introducing an _inductive bias_.\n",
    "That is, we just assume the world works in a specified way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Two different inductive biases\n",
    "\n",
    "Here are two examples of inductive biases we could embed in our learner:\n",
    "1. always predict the majority class - in this case, we predict _the sun will rise_\n",
    "2. always predict the minority class - in this case, we predict _the sun will not rise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Two different worlds\n",
    "\n",
    "Suppose either one of the following is true:\n",
    "1. the sun always rises \n",
    "2. the sun always rises this year, and then it will never rise again\n",
    "\n",
    "Learners using the two proposed biases will have different performance in this learning task, depending on which world they live in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Which one is the better learner?\n",
    "\n",
    "* the most accurate answer is: the one most adept for its own environment\n",
    "* the practical answer is: the one who says the sun will rise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Two (sets of) NFLTs\n",
    "\n",
    "NFLTs formalize these intuitions about the limitations of inductive reasoning.\n",
    "\n",
    "We'll discuss 2 sets of NFLTs:\n",
    "  + Wolpert's: introduced in a paper in 1996\n",
    "  + Shalev-Shwartz & Ben-David's (Shwartz-David, for short): available at their 2014 textbook\n",
    "* Wolpert's are the most commonly cited ones\n",
    "* Shwartz-David's might have more connections to practice though\n",
    "\n",
    "We'll provide formal statements for both - but not proofs :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Shwart & David's NFLT\n",
    "_as well as a simplified definition of PAC-Learnability_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PAC-Learning\n",
    "\n",
    "+ \"PAC\" stands for probably approximately correct\n",
    "+ Leslie Valiant introduced this in 1983\n",
    "+ it was an important part of why he won a Turing Award in 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A (simple) learning problem\n",
    "\n",
    "#### The setting\n",
    "\n",
    "* domain: $X \\in \\mathbb R^p$\n",
    "* output: $Y$\n",
    "* there's an unknown underlying distribution: $P(X, Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Our task\n",
    "\n",
    "* we are given $m$ training cases, sampled i.i.d. (not always realistic)\n",
    "* we must come up with a classifier $h : X \\rightarrow Y$\n",
    "* we want to minimize the loss function $L$ on _unseen data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Minimizing the loss\n",
    "\n",
    "* we can _always_ achieve 0 error on the training set\n",
    "* we know the problem with that: we'll likely _overfit_\n",
    "* that is, our classifier will not generalize to unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### How do we solve this problem?\n",
    "\n",
    "* in PAC-learning, we restrict our search space\n",
    "* we do not consider _every possible classifier_\n",
    "* we only allow classifiers from a (predetermined) _hypothesis class_\n",
    "* for example, we could restrict ourselves to work with _linear models_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### hypothesis class == inductive bias\n",
    "\n",
    "If we choose to only consider linear models,\n",
    "it's as if we were saying:\n",
    "\n",
    "    I believe the underlying distribution is well approximated by a linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The NFLTs are essentially answering:\n",
    "\n",
    "    You do not know that distribution, anything you assume could in principle be wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Empirical Risk Minimization\n",
    "* our strategy for learning is to minimize training error (ERM)\n",
    "* since we are only considering some possible classifiers, we may not achieve 0 training error\n",
    "\n",
    "**Aside**: *it is common to split the dataset into training, cv and test sets. We are ignoring this practice here. It's as though we only had a training set and training error. This doesn't change anything fundamentally in the present discussion.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Is the training error a good approximation for the true error? As it turns out, the answer depends on the hypothesis class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice:\n",
    "* we do not care about having THE best classifier in our hypothesis class\n",
    "* any classifier is fine as long as it is not _too much_ worse than the best one\n",
    "\n",
    "In other words, we want to be close to the best classifier in our hypothesis class.\n",
    "\n",
    "This is what PAC-learnability is essentially: it is likely that our empirically chosen classifier's performance will be close to the best in our class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PAC-learnability\n",
    "\n",
    "Let's name the two classifiers we are concerned with:\n",
    "* $h_S$ is the classifier that has lowest training error\n",
    "* $h_{opt}$ is the classifier that has lowest true error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition** (*PAC-learning*): We say a hypothesis class is *PAC-learnable* if for all $\\epsilon$ and $\\delta$, there is a sample size $m$ above which the following holds:\n",
    "\n",
    "$$ P \\big( (\\text{true error of } h_S) - (\\text{true error of } h_{opt} ) \\le \\epsilon \\big) \\ge 1 - \\delta $$\n",
    "\n",
    "$\\epsilon$ and $\\delta$ are real numbers between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### _Probably Approximately Correct_\n",
    "If our hypothesis class is PAC-learnable, we can safely use the strategy of choosing the classifier with lowest training error:\n",
    "* we'll be *approximately correct*: $\\big( (\\text{true error of } h_S) - (\\text{true error of } h_{opt} ) \\le \\epsilon \\big)$\n",
    "* but we cannot **be sure** we will be approximately correct\n",
    "* we can always get a REALLY BAD training set\n",
    "* what we do know is that we will be *probably* approximately correct: with probability is $1 - \\delta$\n",
    "* if we want to be more correct with higher probability, the required sample size will grow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Learning a PAC-learnable hypothesis class is really convenient: we can just choose the classifier which achieves lowest training error.\n",
    "\n",
    "We might be interested now in asking things like:\n",
    "1. what kinds of hypothesis classes are PAC-learnable? For example, is the set of all linear functions for a regression problem PAC-learnable?\n",
    "2. how big should the sample size be so we can, for example, be 99.9% sure we'll be worse off by at most 1E-10 from the best classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These are important questions. As it turns out, SLT offers answers for both.  \n",
    "\n",
    "The theory of VC-dimension and the fundamental theorem of PAC-Learning provide an answer to the first. \n",
    "And there are formulas to answer the second. \n",
    "\n",
    "But we must move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So, is machine learning solved?\n",
    "#### Does it all boil down to finding a PAC-learnable hypothesis class?\n",
    "**tl;dr:** _no._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We may get very close to the best classifier in our class, but it might still suck.\n",
    "\n",
    "If we apply logistic regression to classify images on ImageNet, the model will likely be bad.\n",
    "Even if it's almost the best possible logistic regression classifier for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### What gives?\n",
    "\n",
    "A restrictive hypothesis class (e.g., logistic regression):\n",
    "* is usually easy to _learn_ (get to the best possible model for that class)\n",
    "* usually sucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is the bias-complexity tradeoff. NFLT, in some sense, is a technical way of saying:\n",
    "\n",
    "    We cannot get rid of the bias. Well, at least if we consider the set of all conceivable problems.\n",
    "    \n",
    "Which, again, is the same as saying we must _assume_ our hypothesis class is good for our particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shwartz-David's theorem\n",
    "\n",
    "#### The setting\n",
    "* binary classification (domain X, labels Y = {0, 1})\n",
    "* our training set cannot cover more than half of the domain\n",
    "  - this is a technicality for _really big_ domains (such as the set of all float32 32x32 images)\n",
    "  - intuition: the half of the domain we _do not_ see could _in principle_ be anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Theorem** (*SD-NFL*): for any learning algorithm we apply and for any fixed training set size we can find a distribution $P(X,Y)$ such that:\n",
    "\n",
    "+ with probability $\\ge$ 1/7 we'll get a training set for which the classifier outputted by the algorithm will have true error $\\ge$ 1/8\n",
    "+ there's a classifier that achieves 0 true error on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Wolpert's NFLTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "+ where are all sorts of extensions and variations of these theorems\n",
    "+ we will see 2 simple though consequential ones\n",
    "+ first, we must learn a slightly different abstraction for a learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Our learning problem\n",
    "\n",
    "#### The setting\n",
    "\n",
    "* domain: $X$\n",
    "* output: $Y$\n",
    "* a known sampling distribution: $\\pi(x)$\n",
    "  - this is how we choose our training examples\n",
    "  - if every person is equally likely to take part in a survey, $\\pi$ is an Uniform distribution over the set of people\n",
    "* labeling function: $f = P(Y | X)$\n",
    "  - how likely are we to see label y associated with input x\n",
    "* dataset: $d = \\{(x_i, y_i) \\text{   for i in range(m)}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A learning algorithm\n",
    "\n",
    "* a learning algorithm outputs $h = P(Y | X)$\n",
    "  - this is simply a labeling function\n",
    "  - we are trying to approximate the true labeling function $f = P(Y | X)$\n",
    "* any algorithm's internal workings can be abstracted as $P(h | d)$\n",
    "  - for all considered classifiers h, this gives the probability of the learning algorithm choosing it after seeing training data d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### The cost C\n",
    "\n",
    "* cost is computed only in $(x, y)$ pairs outside the training set \n",
    "* for those, $C$ is the misclassification rate, ranging from 0 to 1 (0 = no error, 1 = all wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The world is unknowable\n",
    "\n",
    "+ we are trying to approximate $f = P(Y | X)$\n",
    "+ we do not know the true $f$\n",
    "+ we represent our uncertainty regarding it as $P(f)$\n",
    "+ metaphorically, $f$ is the world and $P(f)$ the distribution of possible worlds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wolpert's theorems\n",
    "\n",
    "### Theorem 1\n",
    "\n",
    "The expected cost from any fixed dataset can be written as an inner product between two vectors.\n",
    "One only has to do with our algorithm.\n",
    "The other only has to do with the \"world\" $f$.\n",
    "\n",
    "$E[C | d] = \\langle P(h | d), P(f | d) \\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Consequences of theorem 1\n",
    "\n",
    "##### Performance = approximating P(f)\n",
    "\n",
    "* performance is given by an inner product between\n",
    "  - the learning algorithm $P(h | d)$\n",
    "  - the unknown underlying distribution $P(f | d)$\n",
    "* remember, $\\langle u, v \\rangle = \\| u \\| \\| v \\| cos(\\theta)$\n",
    "* so performance in a sense is a measure of how well a given algorithm matches the true $P(f | d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### No proofs about generalization\n",
    "* we cannot prove from first principles that $P(f | d)$ has any given shape\n",
    "* so we also cannot prove anything about the generalization properties of our algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Theorem 2\n",
    "#### No Free Lunch\n",
    "\n",
    "For any pair of learning algorithms, if we assume an uniform distribution of \"possible worlds\" (uniform $P(f)$), we have:\n",
    "\n",
    "$E_1[C | f, d] = E_2[C | f, d]$\n",
    "\n",
    "--\n",
    "\n",
    "in other words, if all worlds are equally likely, all algorithms have the same performance on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Interpretation of theorem 2\n",
    "\n",
    "##### Why some algorithms do work better than others in practice?\n",
    "\n",
    "+ the theorem depends fundamentally on assuming $P(f)$ is uniform\n",
    "+ we are only focusing on performance on unseen data\n",
    "+ in this setting, performance on the training set means nothing\n",
    "+ uniform $P(f)$ is an unrealistic assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### If uniform $P(f)$ is unrealistic, why assume it anyway?\n",
    "\n",
    "Because uniform $P(f)$ is a *uninformative prior*, a nice way of saying:\n",
    "\n",
    "     I don't want to assume any shape for P(f)\n",
    "     \n",
    "So the theorem essentially says:\n",
    "\n",
    "     You must (implicitly at least) assume something if you want to be confident your algorithm works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "\n",
    "Suppose we are designing learning algorithms. We would like to have an algorithm so good that it will be effective in every conceivable problem. Both of the theorems discussed show that this is not possible.\n",
    "\n",
    "Wolpert's theorem shows that we must at least implicitly assume something about the kinds of problems we will be solving with our algorithm. If we try to be equally good for all possible problems, every algorithm has the same performance. This means that an algorithm that always predicts the same label Y for any input will have the same performance, on average, as the fanciest neural network.\n",
    "\n",
    "In practice, even if we cannot define explicitly $P(f)$, algorithm designers and machine learning practitioners do know implicitly something about it. \n",
    "We know that, because some of the algorithms we use are better than others.\n",
    "\n",
    "Similarly, Shwartz & David's theorem proves that no learning algorithm will be universally good. \n",
    "There will always be some problems for which its performance will be bad.\n",
    "Furthermore, for those problems, there will be some learning algorithms that will have good performance.\n",
    "\n",
    "What these theorems do show is that there is no a priori universally superior supervised learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Further reading & Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The following is an excelent introduction to machine learning from the point of view of SLT:\n",
    "+ Shalev-Shwartz, Ben-David (2004). _Understanding Machine Learning: From Theory to Algorithms_\n",
    "\n",
    "It's also available as a PDF at the [author's website](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf).\n",
    "\n",
    "<img align=\"left\" src=\"book.jpg\" alt=\"Drawing\" style=\"width: 170px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Originally, Wolpert's NFLTs for learning algorithms were introduced in:\n",
    "+ Wolpert (1996). _The Lack of A Priori Distinctions Between Learning Algorithms and The Existence of A Priori Distinctions Between Learning Algorithms_\n",
    "\n",
    "He published many subsequent works discussing the theorems, such as:\n",
    "+ Wolpert (2012). _What the No Free Lunch Theorems Really Mean; How to Improve Search Algorithms_\n",
    "\n",
    "The ones presented here came from their simplified version found (with no proofs) in:\n",
    "+ Wolpert, (2001). _The Supervised Learning No-Free-Lunch Theorems_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a list of websites I consulted:\n",
    "+ https://en.wikipedia.org/wiki/David_Hume#Induction_and_causation\n",
    "+ https://en.wikipedia.org/wiki/Problem_of_induction\n",
    "+ https://mashimo.wordpress.com/2013/03/12/bertrand-russells-inductivist-turkey/\n",
    "+ https://peekaboo-vision.blogspot.com/2019/07/dont-cite-no-free-lunch-theorem.html\n",
    "+ https://amturing.acm.org/award_winners/valiant_2612174.cfm\n",
    "+ http://www.no-free-lunch.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
